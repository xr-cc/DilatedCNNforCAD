{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our mode: the dilated convolution neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import the Sequential model class. \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "#Core layers \n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "\n",
    "#Convolution layers \n",
    "from tensorflow.keras.layers import Conv1D\n",
    "\n",
    "# from tensorflow.keras.layers import Concatenate, concatenate\n",
    "\n",
    "#Pooling layers \n",
    "# from tensorflow.keras.layers import MaxPooling1D, MaxPooling2D, AveragePooling1D, AveragePooling2D \n",
    "\n",
    "#Recurrent layers \n",
    "# from tensorflow.keras.layers import Recurrent, SimpleRNN, GRU, LSTM\n",
    "\n",
    "#Embedding layers \n",
    "# from tensorflow.keras.layers import Embedding\n",
    "\n",
    "#Merge layers \n",
    "# from tensorflow.keras.layers import Add, Multiply, Average, Maximum, Concatenate, Dot\n",
    "\n",
    "#Normalization layers \n",
    "from tensorflow.keras.layers import BatchNormalization \n",
    "\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "num_bp_in_peak = 3000\n",
    "num_bp_padding = 5000\n",
    "down_sample = 1.0\n",
    "margin = num_bp_in_peak/2 + num_bp_padding\n",
    "dir_name = '/data/smooth_muscle_cells'\n",
    "fasta_file = '/data/GRCh38.p3.genome.fa'\n",
    "bigwig_file = '/data/TQ326-pooled_ATAC_USPD16083803_HHHJMBBXX_L4_1.merged.nodup.no_chrM_MT.tn5.fc.signal.bigwig'\n",
    "peak_file = '/data/rep1-pr1_vs_rep1-pr2.overlap.bfilt.narrowPeak.gz'\n",
    "test = 0.1\n",
    "validation = 0.1\n",
    "min_prominence = 0.7\n",
    "\n",
    "# model\n",
    "normalization = True\n",
    "max_peak_val = 5\n",
    "num_filters = 32\n",
    "kernel_size = 24\n",
    "pooling_size = 4\n",
    "dilation_rate = 2\n",
    "dropout_rate = 0.3\n",
    "final_peak_size = 2764\n",
    "\n",
    "# data\n",
    "num_validation_examples = 11988\n",
    "num_test_examples = 10288\n",
    "num_training_examples = 105642 - num_validation_examples - num_test_examples\n",
    "total = 105642\n",
    "\n",
    "# training\n",
    "num_training_examples_in_batch = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new model\n",
    "from tensorflow.python.keras import backend\n",
    "\n",
    "def get_model():\n",
    "#     initial_convolution = Conv1D(num_filters, kernel_size, input_shape=(13000, 4))\n",
    "#     second_convolution = Conv1D(num_filters, kernel_size)\n",
    "#     first_dilation_1 = Conv1D(int(num_filters/2), int(kernel_size/2), dilation_rate=5)\n",
    "#     first_dilation_2 = Conv1D(int(num_filters/2), int(kernel_size/2), dilation_rate=5)\n",
    "#     second_dilation_1 = Conv1D(int(num_filters/4), int(kernel_size/4), dilation_rate=25)\n",
    "#     second_dilation_2 = Conv1D(int(num_filters/4), int(kernel_size/4), dilation_rate=25)\n",
    "#     third_dilation_1 = Conv1D(int(num_filters/8), int(kernel_size/4), dilation_rate=125)\n",
    "#     third_dilation_2 = Conv1D(int(num_filters/8), int(kernel_size/4), dilation_rate=125)\n",
    "#     fourth_dilation_1 = Conv1D(int(num_filters/16), int(kernel_size/4), dilation_rate=256)\n",
    "#     fourth_dilation_2 = Conv1D(int(num_filters/16), int(kernel_size/4), dilation_rate=256)\n",
    "#     fifth_dilation_1 = Conv1D(int(num_filters/32), int(kernel_size/4), dilation_rate=625)\n",
    "#     fifth_dilation_2 = Conv1D(int(num_filters/32), int(kernel_size/4), dilation_rate=625)\n",
    "\n",
    "    initial_convolution = Conv1D(num_filters, kernel_size, input_shape=(13000, 4))\n",
    "    second_convolution = Conv1D(num_filters, kernel_size)\n",
    "    first_dilation_1 = Conv1D(int(num_filters/2), int(kernel_size/6), dilation_rate=5)\n",
    "    first_dilation_2 = Conv1D(int(num_filters/2), int(kernel_size/6), dilation_rate=5)\n",
    "    second_dilation_1 = Conv1D(int(num_filters/4), int(kernel_size/6), dilation_rate=25)\n",
    "    second_dilation_2 = Conv1D(int(num_filters/4), int(kernel_size/6), dilation_rate=25)\n",
    "    add_dilation_1 = Conv1D(int(num_filters/4), int(kernel_size/6), dilation_rate=75)\n",
    "    add_dilation_2 = Conv1D(int(num_filters/4), int(kernel_size/6), dilation_rate=75)\n",
    "    third_dilation_1 = Conv1D(int(num_filters/8), int(kernel_size/4), dilation_rate=125)\n",
    "    third_dilation_2 = Conv1D(int(num_filters/8), int(kernel_size/4), dilation_rate=125)\n",
    "    ann_dilation_1 = Conv1D(int(num_filters/16), int(kernel_size/4), dilation_rate=200)\n",
    "    ann_dilation_2 = Conv1D(int(num_filters/16), int(kernel_size/4), dilation_rate=200)\n",
    "    fourth_dilation_1 = Conv1D(int(num_filters/16), int(kernel_size/4), dilation_rate=256)\n",
    "    fourth_dilation_2 = Conv1D(int(num_filters/16), int(kernel_size/4), dilation_rate=256)\n",
    "    fifth_dilation_1 = Conv1D(int(num_filters/32), int(kernel_size/4), dilation_rate=375)\n",
    "    fifth_dilation_2 = Conv1D(int(num_filters/32), int(kernel_size/4), dilation_rate=375)\n",
    "\n",
    "    input_seq = Input(shape=(13000, 4))\n",
    "\n",
    "    initial_conv = initial_convolution(input_seq)\n",
    "    initial_conv = BatchNormalization()(initial_conv)\n",
    "    initial_conv = Activation('relu')(initial_conv)\n",
    "\n",
    "    second_conv = second_convolution(initial_conv)\n",
    "    second_conv = BatchNormalization()(second_conv)\n",
    "    second_conv = Activation('relu')(second_conv)\n",
    "\n",
    "    with backend.get_graph().as_default():\n",
    "        first_dilation_1.build(second_conv.shape) \n",
    "    first_dilation = first_dilation_1(second_conv)\n",
    "    first_dilation = BatchNormalization()(first_dilation)\n",
    "    first_dilation = Activation('relu')(first_dilation)\n",
    "    print(first_dilation.shape)\n",
    "\n",
    "    with backend.get_graph().as_default():\n",
    "        first_dilation_2.build(first_dilation.shape) \n",
    "    first_dilation = first_dilation_2(first_dilation)\n",
    "    first_dilation = BatchNormalization()(first_dilation)\n",
    "    first_dilation = Activation('relu')(first_dilation)\n",
    "    print(first_dilation.shape)\n",
    "\n",
    "    with backend.get_graph().as_default():\n",
    "        second_dilation_1.build(first_dilation.shape) \n",
    "    second_dilation = second_dilation_1(first_dilation)\n",
    "    second_dilation = BatchNormalization()(second_dilation)\n",
    "    second_dilation = Activation('relu')(second_dilation)\n",
    "    print(second_dilation.shape)\n",
    "\n",
    "    with backend.get_graph().as_default():\n",
    "        second_dilation_2.build(second_dilation.shape) \n",
    "    second_dilation = second_dilation_2(second_dilation)\n",
    "    second_dilation = BatchNormalization()(second_dilation)\n",
    "    second_dilation = Activation('relu')(second_dilation)\n",
    "\n",
    "    print(second_dilation.shape)\n",
    "    \n",
    "    with backend.get_graph().as_default():\n",
    "        add_dilation_1.build(second_dilation.shape) \n",
    "    add_dilation = add_dilation_1(second_dilation)\n",
    "    add_dilation = BatchNormalization()(add_dilation)\n",
    "    add_dilation = Activation('relu')(add_dilation)\n",
    "    print(add_dilation.shape)\n",
    "\n",
    "    with backend.get_graph().as_default():\n",
    "        add_dilation_2.build(add_dilation.shape) \n",
    "    add_dilation = add_dilation_2(add_dilation)\n",
    "    add_dilation = BatchNormalization()(add_dilation)\n",
    "    add_dilation = Activation('relu')(add_dilation)\n",
    "\n",
    "    print(add_dilation.shape)\n",
    "\n",
    "    with backend.get_graph().as_default():\n",
    "        third_dilation_1.build(add_dilation.shape) \n",
    "    third_dilation = third_dilation_1(add_dilation)\n",
    "    third_dilation = BatchNormalization()(third_dilation)\n",
    "    third_dilation = Activation('relu')(third_dilation)\n",
    "    print(third_dilation.shape)\n",
    "\n",
    "    with backend.get_graph().as_default():\n",
    "         third_dilation_2.build(third_dilation.shape) \n",
    "    third_dilation =  third_dilation_2(third_dilation)\n",
    "    third_dilation = BatchNormalization()(third_dilation)\n",
    "    third_dilation = Activation('relu')(third_dilation)\n",
    "\n",
    "    print(third_dilation.shape)\n",
    "    \n",
    "    with backend.get_graph().as_default():\n",
    "        ann_dilation_1.build(third_dilation.shape) \n",
    "    ann_dilation = ann_dilation_1(third_dilation)\n",
    "    ann_dilation = BatchNormalization()(ann_dilation)\n",
    "    ann_dilation = Activation('relu')(ann_dilation)\n",
    "    print(ann_dilation.shape)\n",
    "\n",
    "    with backend.get_graph().as_default():\n",
    "         ann_dilation_2.build(ann_dilation.shape) \n",
    "    ann_dilation =  ann_dilation_2(ann_dilation)\n",
    "    ann_dilation = BatchNormalization()(ann_dilation)\n",
    "    ann_dilation = Activation('relu')(ann_dilation)\n",
    "\n",
    "    print(ann_dilation.shape)\n",
    "\n",
    "    with backend.get_graph().as_default():\n",
    "        fourth_dilation_1.build(ann_dilation.shape) \n",
    "    fourth_dilation = fourth_dilation_1(ann_dilation)\n",
    "    fourth_dilation = BatchNormalization()(fourth_dilation)\n",
    "    fourth_dilation = Activation('relu')(fourth_dilation)\n",
    "    print(fourth_dilation.shape)\n",
    "\n",
    "    with backend.get_graph().as_default():\n",
    "         fourth_dilation_2.build(fourth_dilation.shape) \n",
    "    fourth_dilation =  fourth_dilation_2(fourth_dilation)\n",
    "    fourth_dilation = BatchNormalization()(fourth_dilation)\n",
    "    fourth_dilation = Activation('relu')(fourth_dilation)\n",
    "\n",
    "    print(fourth_dilation.shape)\n",
    "\n",
    "    with backend.get_graph().as_default():\n",
    "        fifth_dilation_1.build(fourth_dilation.shape) \n",
    "    fifth_dilation = fifth_dilation_1(fourth_dilation)\n",
    "    fifth_dilation = BatchNormalization()(fifth_dilation)\n",
    "    fifth_dilation = Activation('relu')(fifth_dilation)\n",
    "    print(fifth_dilation.shape)\n",
    "\n",
    "    with backend.get_graph().as_default():\n",
    "         fifth_dilation_2.build(fifth_dilation.shape) \n",
    "    fifth_dilation =  fifth_dilation_2(fifth_dilation)\n",
    "    fifth_dilation = BatchNormalization()(fifth_dilation)\n",
    "    fifth_dilation = Activation('relu')(fifth_dilation)\n",
    "\n",
    "    print(fifth_dilation.shape)\n",
    "\n",
    "    flatten_output = Flatten()(fifth_dilation)\n",
    "\n",
    "    model = Model(inputs=input_seq, outputs=flatten_output)\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse', 'mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(index_start, index_end):\n",
    "    train_X_files = glob.glob(dir_name+'/sequences/train*')\n",
    "    train_X_files = sorted(train_X_files)\n",
    "    example_file = np.load(train_X_files[0], allow_pickle=True)\n",
    "    num_bp = int(example_file.size/4)\n",
    "    num_examples = index_end - index_start\n",
    "    train_X = np.empty((num_examples, num_bp, 4), dtype=np.float16)\n",
    "    for i in range(num_examples):\n",
    "        train_X_file = train_X_files[i+index_start]\n",
    "        example = np.load(train_X_file, allow_pickle=True)\n",
    "        np.resize(example, (num_bp, 4))\n",
    "        train_X[i,:,:] = np.nan_to_num(example)\n",
    "    print(train_X.shape)\n",
    "    \n",
    "    train_y_files = glob.glob(dir_name+'/peak_data/train*')\n",
    "    train_y_files = sorted(train_y_files)\n",
    "    example_file = np.load(train_y_files[0], allow_pickle=True)\n",
    "    train_y = np.empty((num_examples, example_file.size), dtype=np.float16)\n",
    "    for i in range(num_examples):\n",
    "        train_y_file = train_y_files[i+index_start]\n",
    "        example = np.load(train_y_file, allow_pickle=True)\n",
    "        train_y[i,:] = np.nan_to_num(example)\n",
    "    print(train_y.shape)\n",
    "    \n",
    "    train_X_backward = np.flip(train_X, 2)\n",
    "    train_X = np.concatenate((train_X, train_X_backward), axis=0)\n",
    "    train_y = np.concatenate((train_y, train_y), axis=0)\n",
    "    \n",
    "    return train_X, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_data():\n",
    "    val_X_files = glob.glob(dir_name+'/sequences/validation*')\n",
    "    example_file = np.load(val_X_files[0], allow_pickle=True)\n",
    "    num_bp = int(example_file.size/4)\n",
    "    num_examples = (len(val_X_files))\n",
    "    val_X = np.empty((num_examples, num_bp, 4), dtype=np.float16)\n",
    "    for i in range(num_examples):\n",
    "        val_X_file = val_X_files[i]\n",
    "        example = np.load(val_X_file, allow_pickle=True)\n",
    "        np.resize(example, (num_bp, 4))\n",
    "        val_X[i,:,:] = np.nan_to_num(example)\n",
    "    print(val_X.shape)\n",
    "\n",
    "    \n",
    "    val_y_files = glob.glob(dir_name+'/peak_data/validation*')\n",
    "    example_file = np.load(val_y_files[0], allow_pickle=True)\n",
    "    val_y = np.empty((num_examples, example_file.size), dtype=np.float16) \n",
    "    for i in range(num_examples):\n",
    "        val_y_file = val_y_files[i]\n",
    "        example = np.load(val_y_file, allow_pickle=True)\n",
    "        val_y[i,:] = np.nan_to_num(example)\n",
    "    print(val_y.shape)\n",
    "    \n",
    "    val_X_backward = np.flip(val_X, 2)\n",
    "    val_X = np.concatenate((val_X, val_X_backward), axis=0)\n",
    "    val_y = np.concatenate((val_y, val_y), axis=0)\n",
    "    \n",
    "    return val_X, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testing_data():\n",
    "    test_X_files = glob.glob(dir_name+'/sequences/test*')\n",
    "    example_file = np.load(test_X_files[0], allow_pickle=True)\n",
    "    num_bp = int(example_file.size/4)\n",
    "    num_examples = (len(test_X_files))\n",
    "    test_X = np.empty((num_examples, num_bp, 4), dtype=np.float16)\n",
    "    for i in range(num_examples):\n",
    "        test_X_file = test_X_files[i]\n",
    "        example = np.load(test_X_file, allow_pickle=True)\n",
    "        np.resize(example, (num_bp, 4))\n",
    "        test_X[i,:,:] = np.nan_to_num(example)\n",
    "    print(test_X.shape)\n",
    "    \n",
    "    test_y_files = glob.glob(dir_name+'/peak_data/test*')\n",
    "    example_file = np.load(test_y_files[0], allow_pickle=True)\n",
    "    test_y = np.empty((num_examples, example_file.size), dtype=np.float16)\n",
    "    for i in range(num_examples):\n",
    "        test_y_file = test_y_files[i]\n",
    "        example = np.load(test_y_file, allow_pickle=True)\n",
    "        test_y[i,:] = np.nan_to_num(example)\n",
    "    print(test_y.shape)\n",
    "    \n",
    "    test_X_backward = np.flip(test_X, 2)\n",
    "    test_X = np.concatenate((test_X, test_X_backward), axis=0)\n",
    "    test_y = np.concatenate((test_y, test_y), axis=0)\n",
    "    \n",
    "    return test_X, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 12939, 16)\n",
      "(None, 12924, 16)\n",
      "(None, 12849, 8)\n",
      "(None, 12774, 8)\n",
      "(None, 12549, 8)\n",
      "(None, 12324, 8)\n",
      "(None, 11699, 4)\n",
      "(None, 11074, 4)\n",
      "(None, 10074, 2)\n",
      "(None, 9074, 2)\n",
      "(None, 7794, 2)\n",
      "(None, 6514, 2)\n",
      "(None, 4639, 1)\n",
      "(None, 2764, 1)\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 13000, 4)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 12977, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 12977, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 12977, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 12954, 32)         24608     \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 12954, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 12954, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 12939, 16)         2064      \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 12939, 16)         64        \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 12939, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 12924, 16)         1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 12924, 16)         64        \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 12924, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 12849, 8)          520       \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 12849, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 12849, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 12774, 8)          264       \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 12774, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 12774, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 12549, 8)          264       \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 12549, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 12549, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 12324, 8)          264       \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 12324, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 12324, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 11699, 4)          196       \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 11699, 4)          16        \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 11699, 4)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 11074, 4)          100       \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 11074, 4)          16        \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 11074, 4)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 10074, 2)          50        \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 10074, 2)          8         \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 10074, 2)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 9074, 2)           26        \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 9074, 2)           8         \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 9074, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7794, 2)           26        \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 7794, 2)           8         \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7794, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 6514, 2)           26        \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 6514, 2)           8         \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 6514, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 4639, 1)           13        \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 4639, 1)           4         \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 4639, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 2764, 1)           7         \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 2764, 1)           4         \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 2764, 1)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2764)              0         \n",
      "=================================================================\n",
      "Total params: 33,156\n",
      "Trainable params: 32,864\n",
      "Non-trainable params: 292\n",
      "_________________________________________________________________\n",
      "(10000, 13000, 4)\n",
      "(10000, 3000)\n",
      "Train on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 1303s 65ms/sample - loss: 6.3295 - mse: 6.3295 - mae: 1.6730\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 1303s 65ms/sample - loss: 6.3298 - mse: 6.3298 - mae: 1.6731\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 1302s 65ms/sample - loss: 6.3308 - mse: 6.3308 - mae: 1.6729\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 1338s 67ms/sample - loss: 6.3299 - mse: 6.3299 - mae: 1.6736\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 1349s 67ms/sample - loss: 6.3302 - mse: 6.3302 - mae: 1.6733\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 1351s 68ms/sample - loss: 6.3308 - mse: 6.3308 - mae: 1.6736\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 1341s 67ms/sample - loss: 6.3295 - mse: 6.3295 - mae: 1.6731\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 1309s 65ms/sample - loss: 6.3302 - mse: 6.3302 - mae: 1.6732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 1307s 65ms/sample - loss: 6.3301 - mse: 6.3301 - mae: 1.6735\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 1306s 65ms/sample - loss: 6.3301 - mse: 6.3301 - mae: 1.6737\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 1305s 65ms/sample - loss: 6.3299 - mse: 6.3299 - mae: 1.6726\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 1299s 65ms/sample - loss: 6.3305 - mse: 6.3305 - mae: 1.6739\n",
      "Epoch 15/20\n",
      "13300/20000 [==================>...........] - ETA: 7:13 - loss: 6.3586 - mse: 6.3586 - mae: 1.6768"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 1297s 65ms/sample - loss: 6.3301 - mse: 6.3301 - mae: 1.6731\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 1300s 65ms/sample - loss: 6.3301 - mse: 6.3301 - mae: 1.6737\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 1203s 60ms/sample - loss: 6.3294 - mse: 6.3294 - mae: 1.6735\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 1301s 65ms/sample - loss: 6.3303 - mse: 6.3303 - mae: 1.6731\n",
      "INFO:tensorflow:Assets written to: /data/CNN_expanded_forward_and_back/1/assets\n",
      "(10000, 13000, 4)\n",
      "(10000, 3000)\n",
      "Train on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 1303s 65ms/sample - loss: 6.2324 - mse: 6.2324 - mae: 1.6552\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 1304s 65ms/sample - loss: 6.2324 - mse: 6.2324 - mae: 1.6554\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 1318s 66ms/sample - loss: 6.2321 - mse: 6.2321 - mae: 1.6548\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 1307s 65ms/sample - loss: 6.2320 - mse: 6.2320 - mae: 1.6555\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2208s 110ms/sample - loss: 6.2321 - mse: 6.2321 - mae: 1.6553\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2453s 123ms/sample - loss: 6.2323 - mse: 6.2323 - mae: 1.6549\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2462s 123ms/sample - loss: 6.2318 - mse: 6.2318 - mae: 1.6557\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2448s 122ms/sample - loss: 6.2330 - mse: 6.2330 - mae: 1.6548\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2443s 122ms/sample - loss: 6.2315 - mse: 6.2315 - mae: 1.6551\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 2409s 120ms/sample - loss: 6.2318 - mse: 6.2318 - mae: 1.6554\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 2455s 123ms/sample - loss: 6.2320 - mse: 6.2320 - mae: 1.6552\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 2448s 122ms/sample - loss: 6.2316 - mse: 6.2316 - mae: 1.6551\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 2444s 122ms/sample - loss: 6.2318 - mse: 6.2318 - mae: 1.6552\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 2452s 123ms/sample - loss: 6.2315 - mse: 6.2315 - mae: 1.6555\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 2447s 122ms/sample - loss: 6.2321 - mse: 6.2321 - mae: 1.6549\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 2433s 122ms/sample - loss: 6.2323 - mse: 6.2323 - mae: 1.6551\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 2432s 122ms/sample - loss: 6.2316 - mse: 6.2316 - mae: 1.6556\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 2430s 121ms/sample - loss: 6.2316 - mse: 6.2316 - mae: 1.6551\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 2453s 123ms/sample - loss: 6.2316 - mse: 6.2316 - mae: 1.6549\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 2452s 123ms/sample - loss: 6.2313 - mse: 6.2313 - mae: 1.6551\n",
      "INFO:tensorflow:Assets written to: /data/CNN_expanded_forward_and_back/2/assets\n",
      "(10000, 13000, 4)\n",
      "(10000, 3000)\n",
      "Train on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 1910s 96ms/sample - loss: 6.3854 - mse: 6.3854 - mae: 1.6821\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 1330s 66ms/sample - loss: 6.3855 - mse: 6.3855 - mae: 1.6822\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 1318s 66ms/sample - loss: 6.3854 - mse: 6.3854 - mae: 1.6821\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 1324s 66ms/sample - loss: 6.3854 - mse: 6.3854 - mae: 1.6817\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 1307s 65ms/sample - loss: 6.3853 - mse: 6.3853 - mae: 1.6822\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 1308s 65ms/sample - loss: 6.3852 - mse: 6.3852 - mae: 1.6823\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 1312s 66ms/sample - loss: 6.3861 - mse: 6.3861 - mae: 1.6816\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 1308s 65ms/sample - loss: 6.3853 - mse: 6.3853 - mae: 1.6817\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 1312s 66ms/sample - loss: 6.3852 - mse: 6.3852 - mae: 1.6826\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 1309s 65ms/sample - loss: 6.3856 - mse: 6.3856 - mae: 1.6813\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 1280s 64ms/sample - loss: 6.3856 - mse: 6.3856 - mae: 1.6817\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 1343s 67ms/sample - loss: 6.3855 - mse: 6.3855 - mae: 1.6818\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 1354s 68ms/sample - loss: 6.3852 - mse: 6.3852 - mae: 1.6819\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 1348s 67ms/sample - loss: 6.3856 - mse: 6.3856 - mae: 1.6824\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 1312s 66ms/sample - loss: 6.3858 - mse: 6.3858 - mae: 1.6822\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 1295s 65ms/sample - loss: 6.3854 - mse: 6.3854 - mae: 1.6819\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 1301s 65ms/sample - loss: 6.3853 - mse: 6.3853 - mae: 1.6824\n",
      "Epoch 19/20\n",
      "10650/20000 [==============>...............] - ETA: 10:04 - loss: 6.3668 - mse: 6.3668 - mae: 1.6789"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 1297s 65ms/sample - loss: 6.3853 - mse: 6.3853 - mae: 1.6816\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 1302s 65ms/sample - loss: 6.3860 - mse: 6.3860 - mae: 1.6819\n",
      "INFO:tensorflow:Assets written to: /data/CNN_expanded_forward_and_back/3/assets\n",
      "(10000, 13000, 4)\n",
      "(10000, 3000)\n",
      "Train on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 1296s 65ms/sample - loss: 6.2540 - mse: 6.2540 - mae: 1.6666\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 1199s 60ms/sample - loss: 6.2543 - mse: 6.2543 - mae: 1.6664\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 1296s 65ms/sample - loss: 6.2536 - mse: 6.2536 - mae: 1.6665\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 1296s 65ms/sample - loss: 6.2539 - mse: 6.2539 - mae: 1.6668\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 1299s 65ms/sample - loss: 6.2538 - mse: 6.2538 - mae: 1.6662\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 1300s 65ms/sample - loss: 6.2536 - mse: 6.2536 - mae: 1.6673\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 1291s 65ms/sample - loss: 6.2541 - mse: 6.2541 - mae: 1.6666\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 1303s 65ms/sample - loss: 6.2537 - mse: 6.2537 - mae: 1.6667\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 1328s 66ms/sample - loss: 6.2544 - mse: 6.2544 - mae: 1.6662\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 1326s 66ms/sample - loss: 6.2536 - mse: 6.2536 - mae: 1.6663\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 1298s 65ms/sample - loss: 6.2536 - mse: 6.2536 - mae: 1.6659\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 1184s 59ms/sample - loss: 6.2541 - mse: 6.2541 - mae: 1.6674\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 1297s 65ms/sample - loss: 6.2539 - mse: 6.2539 - mae: 1.6662\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 1299s 65ms/sample - loss: 6.2540 - mse: 6.2540 - mae: 1.6667\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 1295s 65ms/sample - loss: 6.2538 - mse: 6.2538 - mae: 1.6668\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 1297s 65ms/sample - loss: 6.2541 - mse: 6.2541 - mae: 1.6665\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 1297s 65ms/sample - loss: 6.2537 - mse: 6.2537 - mae: 1.6667\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 1299s 65ms/sample - loss: 6.2539 - mse: 6.2539 - mae: 1.6664\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 1298s 65ms/sample - loss: 6.2542 - mse: 6.2542 - mae: 1.6660\n",
      "INFO:tensorflow:Assets written to: /data/CNN_expanded_forward_and_back/4/assets\n",
      "(10000, 13000, 4)\n",
      "(10000, 3000)\n",
      "Train on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 1298s 65ms/sample - loss: 6.3051 - mse: 6.3051 - mae: 1.6721\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 1302s 65ms/sample - loss: 6.3043 - mse: 6.3043 - mae: 1.6717\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 1282s 64ms/sample - loss: 6.3043 - mse: 6.3043 - mae: 1.6712\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 1276s 64ms/sample - loss: 6.3045 - mse: 6.3045 - mae: 1.6728\n",
      "Epoch 5/20\n",
      "19100/20000 [===========================>..] - ETA: 58s - loss: 6.3056 - mse: 6.3056 - mae: 1.6714 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 927s 46ms/sample - loss: 6.3045 - mse: 6.3045 - mae: 1.6720\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 935s 47ms/sample - loss: 6.3049 - mse: 6.3049 - mae: 1.6716\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 934s 47ms/sample - loss: 6.3044 - mse: 6.3044 - mae: 1.6717\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 935s 47ms/sample - loss: 6.3052 - mse: 6.3052 - mae: 1.6720\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 932s 47ms/sample - loss: 6.3049 - mse: 6.3049 - mae: 1.6715\n",
      "INFO:tensorflow:Assets written to: /data/CNN_expanded_forward_and_back/5/assets\n",
      "(10000, 13000, 4)\n",
      "(10000, 3000)\n",
      "Train on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 936s 47ms/sample - loss: 6.2284 - mse: 6.2284 - mae: 1.6582\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 952s 48ms/sample - loss: 6.2287 - mse: 6.2287 - mae: 1.6585\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 941s 47ms/sample - loss: 6.2286 - mse: 6.2286 - mae: 1.6590\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 950s 47ms/sample - loss: 6.2287 - mse: 6.2287 - mae: 1.6590\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 939s 47ms/sample - loss: 6.2287 - mse: 6.2287 - mae: 1.6583\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 984s 49ms/sample - loss: 6.2283 - mse: 6.2283 - mae: 1.6585\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 937s 47ms/sample - loss: 6.2286 - mse: 6.2286 - mae: 1.6584\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 932s 47ms/sample - loss: 6.2285 - mse: 6.2285 - mae: 1.6582\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 932s 47ms/sample - loss: 6.2290 - mse: 6.2290 - mae: 1.6590\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 932s 47ms/sample - loss: 6.2290 - mse: 6.2290 - mae: 1.6589\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 934s 47ms/sample - loss: 6.2283 - mse: 6.2283 - mae: 1.6583\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 938s 47ms/sample - loss: 6.2291 - mse: 6.2291 - mae: 1.6586\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 950s 48ms/sample - loss: 6.2286 - mse: 6.2286 - mae: 1.6585\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 938s 47ms/sample - loss: 6.2285 - mse: 6.2285 - mae: 1.6583\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 943s 47ms/sample - loss: 6.2284 - mse: 6.2284 - mae: 1.6585\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 951s 48ms/sample - loss: 6.2284 - mse: 6.2284 - mae: 1.6587\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 944s 47ms/sample - loss: 6.2288 - mse: 6.2288 - mae: 1.6584\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 943s 47ms/sample - loss: 6.2285 - mse: 6.2285 - mae: 1.6582\n",
      "INFO:tensorflow:Assets written to: /data/CNN_expanded_forward_and_back/6/assets\n",
      "(10000, 13000, 4)\n",
      "(10000, 3000)\n",
      "Train on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 950s 48ms/sample - loss: 6.3274 - mse: 6.3274 - mae: 1.6738\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 953s 48ms/sample - loss: 6.3272 - mse: 6.3272 - mae: 1.6739\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 957s 48ms/sample - loss: 6.3274 - mse: 6.3274 - mae: 1.6729\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 959s 48ms/sample - loss: 6.3271 - mse: 6.3271 - mae: 1.6733\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 950s 47ms/sample - loss: 6.3273 - mse: 6.3273 - mae: 1.6739\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 945s 47ms/sample - loss: 6.3272 - mse: 6.3272 - mae: 1.6734\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 939s 47ms/sample - loss: 6.3271 - mse: 6.3271 - mae: 1.6741\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 950s 48ms/sample - loss: 6.3273 - mse: 6.3273 - mae: 1.6732\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 951s 48ms/sample - loss: 6.3272 - mse: 6.3272 - mae: 1.6732\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 949s 47ms/sample - loss: 6.3273 - mse: 6.3273 - mae: 1.6738\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 946s 47ms/sample - loss: 6.3270 - mse: 6.3270 - mae: 1.6737\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 950s 47ms/sample - loss: 6.3270 - mse: 6.3270 - mae: 1.6734\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 961s 48ms/sample - loss: 6.3272 - mse: 6.3272 - mae: 1.6735\n",
      "Epoch 14/20\n",
      " 5450/20000 [=======>......................] - ETA: 11:30 - loss: 6.4499 - mse: 6.4499 - mae: 1.6857"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 946s 47ms/sample - loss: 6.3270 - mse: 6.3270 - mae: 1.6733\n",
      "INFO:tensorflow:Assets written to: /data/CNN_expanded_forward_and_back/7/assets\n",
      "(10000, 13000, 4)\n",
      "(10000, 3000)\n",
      "Train on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 941s 47ms/sample - loss: 6.2765 - mse: 6.2765 - mae: 1.6660\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 940s 47ms/sample - loss: 6.2766 - mse: 6.2766 - mae: 1.6663\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 938s 47ms/sample - loss: 6.2767 - mse: 6.2767 - mae: 1.6664\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 934s 47ms/sample - loss: 6.2765 - mse: 6.2765 - mae: 1.6663\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 938s 47ms/sample - loss: 6.2765 - mse: 6.2765 - mae: 1.6663\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 939s 47ms/sample - loss: 6.2764 - mse: 6.2764 - mae: 1.6655\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 940s 47ms/sample - loss: 6.2767 - mse: 6.2767 - mae: 1.6665\n",
      "Epoch 8/20\n",
      " 1150/20000 [>.............................] - ETA: 14:41 - loss: 6.0397 - mse: 6.0397 - mae: 1.6288"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 936s 47ms/sample - loss: 6.2770 - mse: 6.2770 - mae: 1.6659\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 940s 47ms/sample - loss: 6.2763 - mse: 6.2763 - mae: 1.6663\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 935s 47ms/sample - loss: 6.2766 - mse: 6.2766 - mae: 1.6662\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 941s 47ms/sample - loss: 6.2767 - mse: 6.2767 - mae: 1.6660\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 938s 47ms/sample - loss: 6.2762 - mse: 6.2762 - mae: 1.6667\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 940s 47ms/sample - loss: 6.2768 - mse: 6.2768 - mae: 1.6658\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 940s 47ms/sample - loss: 6.2766 - mse: 6.2766 - mae: 1.6667\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 930s 46ms/sample - loss: 6.2767 - mse: 6.2767 - mae: 1.6656\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 937s 47ms/sample - loss: 6.2765 - mse: 6.2765 - mae: 1.6664\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 941s 47ms/sample - loss: 6.2764 - mse: 6.2764 - mae: 1.6657\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 940s 47ms/sample - loss: 6.2765 - mse: 6.2765 - mae: 1.6664\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 940s 47ms/sample - loss: 6.2767 - mse: 6.2767 - mae: 1.6661\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 935s 47ms/sample - loss: 6.2766 - mse: 6.2766 - mae: 1.6661\n",
      "INFO:tensorflow:Assets written to: /data/CNN_expanded_forward_and_back/8/assets\n",
      "(3366, 13000, 4)\n",
      "(3366, 3000)\n",
      "Train on 6732 samples\n",
      "Epoch 1/20\n",
      "6732/6732 [==============================] - 321s 48ms/sample - loss: 6.3480 - mse: 6.3480 - mae: 1.6740\n",
      "Epoch 2/20\n",
      "6732/6732 [==============================] - 311s 46ms/sample - loss: 6.3486 - mse: 6.3486 - mae: 1.6709\n",
      "Epoch 3/20\n",
      "6732/6732 [==============================] - 314s 47ms/sample - loss: 6.3483 - mse: 6.3483 - mae: 1.6727\n",
      "Epoch 4/20\n",
      "6732/6732 [==============================] - 313s 46ms/sample - loss: 6.3479 - mse: 6.3479 - mae: 1.6734\n",
      "Epoch 5/20\n",
      "6732/6732 [==============================] - 315s 47ms/sample - loss: 6.3486 - mse: 6.3486 - mae: 1.6721\n",
      "Epoch 6/20\n",
      "6732/6732 [==============================] - 310s 46ms/sample - loss: 6.3487 - mse: 6.3487 - mae: 1.6706\n",
      "Epoch 7/20\n",
      "6732/6732 [==============================] - 313s 46ms/sample - loss: 6.3485 - mse: 6.3485 - mae: 1.6718\n",
      "Epoch 8/20\n",
      "6732/6732 [==============================] - 311s 46ms/sample - loss: 6.3475 - mse: 6.3475 - mae: 1.6748\n",
      "Epoch 9/20\n",
      "6732/6732 [==============================] - 311s 46ms/sample - loss: 6.3486 - mse: 6.3486 - mae: 1.6712\n",
      "Epoch 10/20\n",
      "6732/6732 [==============================] - 315s 47ms/sample - loss: 6.3478 - mse: 6.3478 - mae: 1.6726\n",
      "Epoch 11/20\n",
      "6732/6732 [==============================] - 313s 47ms/sample - loss: 6.3482 - mse: 6.3482 - mae: 1.6730\n",
      "Epoch 12/20\n",
      "6732/6732 [==============================] - 309s 46ms/sample - loss: 6.3488 - mse: 6.3488 - mae: 1.6711\n",
      "Epoch 13/20\n",
      "6732/6732 [==============================] - 311s 46ms/sample - loss: 6.3482 - mse: 6.3482 - mae: 1.6714\n",
      "Epoch 14/20\n",
      "6732/6732 [==============================] - 310s 46ms/sample - loss: 6.3480 - mse: 6.3479 - mae: 1.6727\n",
      "Epoch 15/20\n",
      "6732/6732 [==============================] - 312s 46ms/sample - loss: 6.3475 - mse: 6.3475 - mae: 1.6718\n",
      "Epoch 16/20\n",
      "6732/6732 [==============================] - 311s 46ms/sample - loss: 6.3483 - mse: 6.3483 - mae: 1.6732\n",
      "Epoch 17/20\n",
      "6732/6732 [==============================] - 311s 46ms/sample - loss: 6.3490 - mse: 6.3490 - mae: 1.6699\n",
      "Epoch 18/20\n",
      "6732/6732 [==============================] - 311s 46ms/sample - loss: 6.3481 - mse: 6.3481 - mae: 1.6731\n",
      "Epoch 19/20\n",
      "6732/6732 [==============================] - 310s 46ms/sample - loss: 6.3478 - mse: 6.3478 - mae: 1.6733\n",
      "Epoch 20/20\n",
      "6732/6732 [==============================] - 310s 46ms/sample - loss: 6.3481 - mse: 6.3481 - mae: 1.6723\n",
      "INFO:tensorflow:Assets written to: /data/CNN_expanded_forward_and_back/9/assets\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.saved_model import save\n",
    "model = get_model()\n",
    "start_index = 0\n",
    "end_index = num_training_examples_in_batch\n",
    "index = 1\n",
    "while start_index < num_training_examples:\n",
    "    train_x, train_y = get_training_data(start_index, end_index)\n",
    "    model.fit(train_x, train_y[:, 118:118+final_peak_size], epochs=20, batch_size=50, verbose=1)\n",
    "    save(model, \"/data/CNN_expanded_forward_and_back/\"+str(index)+\"/\")\n",
    "    start_index = end_index\n",
    "    end_index += num_training_examples_in_batch\n",
    "    if end_index > num_training_examples:\n",
    "        end_index = num_training_examples\n",
    "    index += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4519, 13000, 4)\n",
      "(4519, 3000)\n"
     ]
    }
   ],
   "source": [
    "val_X, val_y = get_validation_data()\n",
    "results = model.evaluate(val_X, val_y[:, 118:118+final_peak_size], batch_size=50)\n",
    "preds = model.predict(val_X)\n",
    "print(pearsonr(np.ravel(preds), np.ravel(val_y[:, 118:118+final_peak_size])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spearmanr(np.ravel(preds), np.ravel(val_y[:, 118:118+final_peak_size])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print(r2_score(np.ravel(preds), np.ravel(val_y[:, 118:118+final_peak_size])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(np.ravel(val_y[:, 118:118+final_peak_size]), np.ravel(preds))\n",
    "plt.title('Dilated Convolution Model')\n",
    "plt.xlabel('true y')\n",
    "plt.ylabel('predicted y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pysam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load list of top SNPs\n",
    "snp_dir_name = \"/data/list_of_snps/\"\n",
    "output_dir_name = '/data/forward_only_expanded_conv_output/'\n",
    "data = pd.read_csv(snp_dir_name+\"TOP_1000_SNPS.txt\",delim_whitespace=True)\n",
    "saved_dir_name = dir_name = '/data/top1000snps/'\n",
    "index_list = data.index.values.tolist()\n",
    "snp_to_max_val = {}\n",
    "for i in range(0,len(index_list)):\n",
    "    snp = data.loc[index_list[i],:]\n",
    "    ref_encoding = np.load(saved_dir_name +\"snp\"+str(i)+\"_ref.npy\", allow_pickle=True) \n",
    "    alt_encoding = np.load(saved_dir_name +\"snp\"+str(i)+\"_alt.npy\", allow_pickle=True) \n",
    "    combined = np.stack((ref_encoding, alt_encoding), axis=0)\n",
    "    preds = model.predict(combined)\n",
    "    diff = preds[0] - preds[1]\n",
    "    diff = np.absolute(diff)\n",
    "    max_diff = np.amax(diff)\n",
    "    np.save(output_dir_name+snp['SNP']+'_'+snp['reference_allele']+'_'+snp['other_allele']+'_reference', preds[0], allow_pickle=True)\n",
    "    np.save(output_dir_name+snp['SNP']+'_'+snp['reference_allele']+'_'+snp['other_allele']+'_alt', preds[1], allow_pickle=True) \n",
    "    np.save(output_dir_name+snp['SNP']+'_'+snp['reference_allele']+'_'+snp['other_allele']+'_diff', diff, allow_pickle=True) \n",
    "    snp_to_max_val[snp['SNP']] = max_diff\n",
    "    if i%100 == 0:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rs16988472', 3.1300192),\n",
       " ('rs6922782', 3.1195364),\n",
       " ('rs17608766', 2.915369),\n",
       " ('rs973754', 2.7724278),\n",
       " ('rs923864', 2.6584415),\n",
       " ('rs11072784', 2.301548),\n",
       " ('rs17476297', 2.2163095),\n",
       " ('rs6917097', 2.1828995),\n",
       " ('rs12327659', 2.0349503),\n",
       " ('rs1527797', 1.9637203),\n",
       " ('rs1126642', 1.7468817),\n",
       " ('rs8024939', 1.6831379),\n",
       " ('rs10496739', 1.6675731),\n",
       " ('rs1537340', 1.6648457),\n",
       " ('rs11022154', 1.5081453),\n",
       " ('rs732998', 1.4483705),\n",
       " ('rs10115049', 1.4478157),\n",
       " ('rs1199330', 1.4353504),\n",
       " ('rs11204085', 1.4064875),\n",
       " ('rs2292334', 1.3865337),\n",
       " ('rs7634457', 1.3860179),\n",
       " ('rs1810126', 1.3678312),\n",
       " ('rs1713806', 1.3285065),\n",
       " ('rs271', 1.3245583),\n",
       " ('rs11926093', 1.301243),\n",
       " ('rs7341786', 1.2395201),\n",
       " ('rs3779787', 1.1835718),\n",
       " ('rs7617509', 1.1793585),\n",
       " ('rs11619113', 1.177341),\n",
       " ('rs513391', 1.1742),\n",
       " ('rs6725887', 1.1592771),\n",
       " ('rs9818870', 1.1584806),\n",
       " ('rs9832013', 1.1448612),\n",
       " ('rs4970834', 1.1423395),\n",
       " ('rs12203818', 1.1354165),\n",
       " ('rs11066028', 1.115442),\n",
       " ('rs3816052', 1.1106443),\n",
       " ('rs7214002', 1.0929799),\n",
       " ('rs9633712', 1.0701213),\n",
       " ('rs1122608', 1.0596905),\n",
       " ('rs6458461', 1.0333691),\n",
       " ('rs16905518', 1.0322883),\n",
       " ('rs4977575', 1.0305328),\n",
       " ('rs6782181', 1.0267334),\n",
       " ('rs7979478', 1.0118266),\n",
       " ('rs2760745', 1.0052843),\n",
       " ('rs11887534', 0.9958751),\n",
       " ('rs518394', 0.9802065),\n",
       " ('rs6583288', 0.9622729),\n",
       " ('rs1343322', 0.9458122),\n",
       " ('rs2178185', 0.9450872),\n",
       " ('rs6502619', 0.9304728),\n",
       " ('rs7310409', 0.92084694),\n",
       " ('rs4773144', 0.90657806),\n",
       " ('rs1727875', 0.8989403),\n",
       " ('rs10883832', 0.8977752),\n",
       " ('rs1746049', 0.89201),\n",
       " ('rs4341796', 0.88658285),\n",
       " ('rs7578326', 0.88405645),\n",
       " ('rs1333049', 0.8829732),\n",
       " ('rs4680124', 0.8765533),\n",
       " ('rs4977574', 0.86257887),\n",
       " ('rs3794632', 0.86073303),\n",
       " ('rs13059788', 0.86072457),\n",
       " ('rs301', 0.85980034),\n",
       " ('rs10811650', 0.85862553),\n",
       " ('rs10965215', 0.8580363),\n",
       " ('rs6984210', 0.85454464),\n",
       " ('rs11191548', 0.85364556),\n",
       " ('rs4386452', 0.8366976),\n",
       " ('rs11724647', 0.829932),\n",
       " ('rs4887075', 0.827754),\n",
       " ('rs12924776', 0.8267007),\n",
       " ('rs687175', 0.82597065),\n",
       " ('rs6453645', 0.81223106),\n",
       " ('rs6743414', 0.7963717),\n",
       " ('rs4793992', 0.793607),\n",
       " ('rs16950046', 0.7823262),\n",
       " ('rs9603710', 0.7617171),\n",
       " ('rs2505084', 0.7538805),\n",
       " ('rs1247351', 0.744122),\n",
       " ('rs11633519', 0.7305248),\n",
       " ('rs604674', 0.7264147),\n",
       " ('rs12217501', 0.7147753),\n",
       " ('rs9319288', 0.712579),\n",
       " ('rs7219320', 0.7049426),\n",
       " ('rs7775710', 0.70253134),\n",
       " ('rs1953088', 0.686247),\n",
       " ('rs9978407', 0.68106914),\n",
       " ('rs9521731', 0.67151546),\n",
       " ('rs12919314', 0.66138387),\n",
       " ('rs487465', 0.66073954),\n",
       " ('rs16905599', 0.653306),\n",
       " ('rs1515098', 0.65256596),\n",
       " ('rs13201878', 0.6501131),\n",
       " ('rs11191514', 0.64426136),\n",
       " ('rs2146238', 0.64266443),\n",
       " ('rs1108648', 0.6398773),\n",
       " ('rs3127591', 0.63823843),\n",
       " ('rs2172593', 0.6272576),\n",
       " ('rs11645860', 0.6267605),\n",
       " ('rs1333042', 0.6256876),\n",
       " ('rs7166158', 0.622231),\n",
       " ('rs2302783', 0.6174917),\n",
       " ('rs3218020', 0.6119094),\n",
       " ('rs4925120', 0.60991096),\n",
       " ('rs216172', 0.6062889),\n",
       " ('rs2119689', 0.6053566),\n",
       " ('rs9472752', 0.6050732),\n",
       " ('rs2297508', 0.6040381),\n",
       " ('rs2242664', 0.6039183),\n",
       " ('rs4299203', 0.6009197),\n",
       " ('rs12600546', 0.59864855),\n",
       " ('rs3861181', 0.59463406),\n",
       " ('rs11751605', 0.59455717),\n",
       " ('rs11191515', 0.58814526),\n",
       " ('rs1316361', 0.5805023),\n",
       " ('rs2304230', 0.5789964),\n",
       " ('rs2285782', 0.5788852),\n",
       " ('rs2157719', 0.57616615),\n",
       " ('rs3824755', 0.5698085),\n",
       " ('rs1533837', 0.5631218),\n",
       " ('rs4887091', 0.56223965),\n",
       " ('rs199624', 0.55925536),\n",
       " ('rs2277547', 0.5553553),\n",
       " ('rs10744770', 0.5546632),\n",
       " ('rs7795121', 0.55105567),\n",
       " ('rs17115213', 0.5510241),\n",
       " ('rs6453649', 0.55001354),\n",
       " ('rs543830', 0.5478282),\n",
       " ('rs12219027', 0.54544544),\n",
       " ('rs11191499', 0.54525924),\n",
       " ('rs771618', 0.54345775),\n",
       " ('rs10757272', 0.54089165),\n",
       " ('rs6453646', 0.5399914),\n",
       " ('rs2252641', 0.538579),\n",
       " ('rs10853104', 0.5370662),\n",
       " ('rs6453644', 0.53162897),\n",
       " ('rs4921914', 0.53061986),\n",
       " ('rs3758348', 0.52756333),\n",
       " ('rs4714866', 0.5273304),\n",
       " ('rs7970490', 0.5273297),\n",
       " ('rs2984941', 0.527051),\n",
       " ('rs11191425', 0.5259516),\n",
       " ('rs11191582', 0.5259447),\n",
       " ('rs9351816', 0.52579045),\n",
       " ('rs7859727', 0.5248941),\n",
       " ('rs11922904', 0.5223528),\n",
       " ('rs2523414', 0.5177336),\n",
       " ('rs3824754', 0.51544476),\n",
       " ('rs1332847', 0.51451993),\n",
       " ('rs12936587', 0.5047324),\n",
       " ('rs915083', 0.50104),\n",
       " ('rs6495312', 0.49676514),\n",
       " ('rs11072787', 0.49434566),\n",
       " ('rs17094683', 0.49368024),\n",
       " ('rs12709088', 0.4906113),\n",
       " ('rs10470171', 0.4896841),\n",
       " ('rs3779788', 0.48799896),\n",
       " ('rs1591136', 0.48757744),\n",
       " ('rs11737895', 0.48531938),\n",
       " ('rs11072794', 0.48398423),\n",
       " ('rs4539564', 0.48231936),\n",
       " ('rs9283839', 0.48231792),\n",
       " ('rs535176', 0.48136616),\n",
       " ('rs12873154', 0.47979033),\n",
       " ('rs1412829', 0.4768101),\n",
       " ('rs4679727', 0.47680688),\n",
       " ('rs12900519', 0.47546864),\n",
       " ('rs17775174', 0.47358263),\n",
       " ('rs964184', 0.46792448),\n",
       " ('rs2876303', 0.46761268),\n",
       " ('rs10850031', 0.46541476),\n",
       " ('rs646776', 0.46437716),\n",
       " ('rs2230590', 0.4631524),\n",
       " ('rs7819541', 0.46301675),\n",
       " ('rs13074317', 0.46111417),\n",
       " ('rs17114036', 0.45510197),\n",
       " ('rs1892225', 0.45418167),\n",
       " ('rs3103349', 0.45215142),\n",
       " ('rs1537371', 0.4513181),\n",
       " ('rs7953150', 0.44960308),\n",
       " ('rs591809', 0.44942307),\n",
       " ('rs216190', 0.44653708),\n",
       " ('rs1926032', 0.44255757),\n",
       " ('rs12554508', 0.44126904),\n",
       " ('rs10029150', 0.43984187),\n",
       " ('rs648997', 0.4378829),\n",
       " ('rs9375991', 0.4364568),\n",
       " ('rs11191531', 0.43633294),\n",
       " ('rs11656840', 0.43472862),\n",
       " ('rs1357574', 0.43431675),\n",
       " ('rs607760', 0.43292022),\n",
       " ('rs6938417', 0.4292438),\n",
       " ('rs454516', 0.42515182),\n",
       " ('rs4925094', 0.42364222),\n",
       " ('rs12984160', 0.42338157),\n",
       " ('rs4637851', 0.42173803),\n",
       " ('rs1486596', 0.42150116),\n",
       " ('rs10965227', 0.4213009),\n",
       " ('rs3786727', 0.42005682),\n",
       " ('rs3752958', 0.41929102),\n",
       " ('rs10802698', 0.41671944),\n",
       " ('rs1713827', 0.4166994),\n",
       " ('rs11617955', 0.41049266),\n",
       " ('rs12714182', 0.40983975),\n",
       " ('rs7078248', 0.4093039),\n",
       " ('rs1457011', 0.40907335),\n",
       " ('rs7808424', 0.4074521),\n",
       " ('rs1892233', 0.4063096),\n",
       " ('rs1265151', 0.4019345),\n",
       " ('rs2641440', 0.4015839),\n",
       " ('rs2891168', 0.40156627),\n",
       " ('rs11852830', 0.4012537),\n",
       " ('rs16955286', 0.40082097),\n",
       " ('rs1661716', 0.39941525),\n",
       " ('rs4741923', 0.3980484),\n",
       " ('rs1332845', 0.3978951),\n",
       " ('rs4077828', 0.39592946),\n",
       " ('rs5029904', 0.3953781),\n",
       " ('rs8045263', 0.3945235),\n",
       " ('rs7769879', 0.39136362),\n",
       " ('rs10738609', 0.39123893),\n",
       " ('rs11867782', 0.38287243),\n",
       " ('rs16911227', 0.3810768),\n",
       " ('rs6705330', 0.37925425),\n",
       " ('rs2078851', 0.37615967),\n",
       " ('rs10738604', 0.3760605),\n",
       " ('rs13079209', 0.37535763),\n",
       " ('rs9581678', 0.3728702),\n",
       " ('rs545226', 0.37243187),\n",
       " ('rs9395214', 0.3703413),\n",
       " ('rs6435169', 0.36982393),\n",
       " ('rs255', 0.36791646),\n",
       " ('rs622956', 0.3678298),\n",
       " ('rs9389108', 0.36626315),\n",
       " ('rs4887074', 0.36619294),\n",
       " ('rs11078406', 0.3661251),\n",
       " ('rs2243547', 0.36541796),\n",
       " ('rs10757264', 0.36157024),\n",
       " ('rs3922879', 0.36083686),\n",
       " ('rs6573987', 0.3596754),\n",
       " ('rs4714955', 0.35946453),\n",
       " ('rs2058919', 0.35669017),\n",
       " ('rs1456896', 0.3539257),\n",
       " ('rs4711858', 0.35366297),\n",
       " ('rs4921913', 0.35317552),\n",
       " ('rs7952972', 0.3522377),\n",
       " ('rs11651451', 0.35146722),\n",
       " ('rs2292954', 0.35093164),\n",
       " ('rs4925093', 0.35083064),\n",
       " ('rs9848655', 0.35055923),\n",
       " ('rs16911234', 0.34997606),\n",
       " ('rs579058', 0.34993744),\n",
       " ('rs12052058', 0.3477931),\n",
       " ('rs6474069', 0.34727693),\n",
       " ('rs11065987', 0.34710693),\n",
       " ('rs16911239', 0.34550905),\n",
       " ('rs1180610', 0.34467697),\n",
       " ('rs17696736', 0.34426475),\n",
       " ('rs3825041', 0.34403586),\n",
       " ('rs3818717', 0.34402394),\n",
       " ('rs7582720', 0.34386933),\n",
       " ('rs7973104', 0.34085536),\n",
       " ('rs1029212', 0.33905816),\n",
       " ('rs4767364', 0.33731198),\n",
       " ('rs4236093', 0.33608985),\n",
       " ('rs11656665', 0.33530593),\n",
       " ('rs7207821', 0.33516926),\n",
       " ('rs944797', 0.33349776),\n",
       " ('rs12951347', 0.33338702),\n",
       " ('rs7503334', 0.33244562),\n",
       " ('rs4506969', 0.33077425),\n",
       " ('rs2505083', 0.3297522),\n",
       " ('rs634537', 0.32327175),\n",
       " ('rs495828', 0.3223917),\n",
       " ('rs523096', 0.321455),\n",
       " ('rs17411126', 0.31919074),\n",
       " ('rs7859362', 0.3191191),\n",
       " ('rs11643271', 0.3181007),\n",
       " ('rs9978142', 0.3179649),\n",
       " ('rs3826714', 0.31659055),\n",
       " ('rs6068963', 0.31598184),\n",
       " ('rs5930', 0.31560802),\n",
       " ('rs6905288', 0.31363153),\n",
       " ('rs8023822', 0.3130746),\n",
       " ('rs17679501', 0.31238914),\n",
       " ('rs13324341', 0.3123622),\n",
       " ('rs6844379', 0.3105873),\n",
       " ('rs934287', 0.30766702),\n",
       " ('rs12412038', 0.30733562),\n",
       " ('rs12205331', 0.30715114),\n",
       " ('rs11191555', 0.30579388),\n",
       " ('rs9381500', 0.30568075),\n",
       " ('rs256', 0.30495727),\n",
       " ('rs9895551', 0.30469227),\n",
       " ('rs8032156', 0.3039899),\n",
       " ('rs529565', 0.30296326),\n",
       " ('rs12828640', 0.3029579),\n",
       " ('rs1504501', 0.30281472),\n",
       " ('rs12435918', 0.30176115),\n",
       " ('rs9866277', 0.30153942),\n",
       " ('rs1333047', 0.3012476),\n",
       " ('rs2383206', 0.30097842),\n",
       " ('rs1565763', 0.30078077),\n",
       " ('rs8076939', 0.3005234),\n",
       " ('rs12980942', 0.30025005),\n",
       " ('rs3105748', 0.29973245),\n",
       " ('rs12994034', 0.29786438),\n",
       " ('rs10458729', 0.29470408),\n",
       " ('rs505922', 0.29290456),\n",
       " ('rs9902941', 0.2921424),\n",
       " ('rs3790515', 0.29107213),\n",
       " ('rs4711857', 0.28900337),\n",
       " ('rs4325552', 0.2866975),\n",
       " ('rs17062983', 0.28620172),\n",
       " ('rs10965219', 0.28566766),\n",
       " ('rs10421990', 0.28496313),\n",
       " ('rs11059465', 0.28422457),\n",
       " ('rs4413022', 0.283679),\n",
       " ('rs7639011', 0.2820778),\n",
       " ('rs1412834', 0.28190494),\n",
       " ('rs943037', 0.27975345),\n",
       " ('rs6502624', 0.27924347),\n",
       " ('rs4925115', 0.2775643),\n",
       " ('rs651007', 0.27557087),\n",
       " ('rs9472790', 0.27500618),\n",
       " ('rs2347252', 0.27479225),\n",
       " ('rs771619', 0.27475262),\n",
       " ('rs10786736', 0.27467507),\n",
       " ('rs1971819', 0.27408457),\n",
       " ('rs6502629', 0.27380592),\n",
       " ('rs7182103', 0.27254033),\n",
       " ('rs749240', 0.2713585),\n",
       " ('rs9381462', 0.27065408),\n",
       " ('rs8035039', 0.27026772),\n",
       " ('rs3809346', 0.26957893),\n",
       " ('rs17514846', 0.26923406),\n",
       " ('rs6841473', 0.26879644),\n",
       " ('rs765548', 0.26811397),\n",
       " ('rs3094379', 0.26729083),\n",
       " ('rs11166641', 0.26644844),\n",
       " ('rs8080823', 0.26384115),\n",
       " ('rs6458545', 0.26367235),\n",
       " ('rs13219256', 0.26290846),\n",
       " ('rs564398', 0.26277637),\n",
       " ('rs10139819', 0.2617222),\n",
       " ('rs1994970', 0.26170492),\n",
       " ('rs2306374', 0.26072085),\n",
       " ('rs1852684', 0.2600689),\n",
       " ('rs923862', 0.26004887),\n",
       " ('rs17630235', 0.25950074),\n",
       " ('rs2834435', 0.259261),\n",
       " ('rs3184504', 0.257959),\n",
       " ('rs2165556', 0.25696588),\n",
       " ('rs6495337', 0.25518662),\n",
       " ('rs9375986', 0.2544676),\n",
       " ('rs12943416', 0.25352395),\n",
       " ('rs12570314', 0.25304723),\n",
       " ('rs2383207', 0.2530176),\n",
       " ('rs1333039', 0.25219727),\n",
       " ('rs503859', 0.25170922),\n",
       " ('rs7217226', 0.2516389),\n",
       " ('rs3106162', 0.2512126),\n",
       " ('rs17471624', 0.25063777),\n",
       " ('rs9864898', 0.25051945),\n",
       " ('rs2257764', 0.24699473),\n",
       " ('rs741334', 0.24688101),\n",
       " ('rs4888422', 0.24667215),\n",
       " ('rs10733376', 0.24591327),\n",
       " ('rs1333043', 0.2458973),\n",
       " ('rs11803940', 0.24539089),\n",
       " ('rs3120139', 0.24301529),\n",
       " ('rs9890064', 0.24100947),\n",
       " ('rs1852687', 0.23908854),\n",
       " ('rs13095258', 0.23894954),\n",
       " ('rs7212249', 0.23856479),\n",
       " ('rs2104332', 0.23809254),\n",
       " ('rs3796581', 0.23806614),\n",
       " ('rs653178', 0.23675191),\n",
       " ('rs4953023', 0.23660707),\n",
       " ('rs2327621', 0.23649746),\n",
       " ('rs1441762', 0.23624575),\n",
       " ('rs3125055', 0.23502529),\n",
       " ('rs1541853', 0.23426795),\n",
       " ('rs1617481', 0.23406267),\n",
       " ('rs2219939', 0.23305988),\n",
       " ('rs13073025', 0.23288226),\n",
       " ('rs2238151', 0.2328372),\n",
       " ('rs13143871', 0.23041344),\n",
       " ('rs6799597', 0.22993088),\n",
       " ('rs7182716', 0.22933412),\n",
       " ('rs8180628', 0.22882694),\n",
       " ('rs13096479', 0.22822171),\n",
       " ('rs2576354', 0.22795498),\n",
       " ('rs1563966', 0.22745037),\n",
       " ('rs4766558', 0.22735941),\n",
       " ('rs3825807', 0.2261033),\n",
       " ('rs13211739', 0.22566485),\n",
       " ('rs481294', 0.22562683),\n",
       " ('rs10811644', 0.22561455),\n",
       " ('rs10965224', 0.22533882),\n",
       " ('rs7217687', 0.22508064),\n",
       " ('rs2410623', 0.2248944),\n",
       " ('rs629301', 0.22380593),\n",
       " ('rs2174914', 0.22342277),\n",
       " ('rs16905524', 0.22340155),\n",
       " ('rs4711863', 0.22246003),\n",
       " ('rs9514898', 0.2206651),\n",
       " ('rs1581675', 0.22035909),\n",
       " ('rs12943566', 0.21969706),\n",
       " ('rs2285810', 0.21958065),\n",
       " ('rs6475608', 0.2195046),\n",
       " ('rs2410618', 0.2190659),\n",
       " ('rs7418501', 0.2187345),\n",
       " ('rs16893523', 0.21821034),\n",
       " ('rs216219', 0.21732259),\n",
       " ('rs3732837', 0.21618873),\n",
       " ('rs7224619', 0.2160821),\n",
       " ('rs1412832', 0.21574306),\n",
       " ('rs9296512', 0.21546471),\n",
       " ('rs2047009', 0.21517754),\n",
       " ('rs1495741', 0.21471786),\n",
       " ('rs1571997', 0.21450233),\n",
       " ('rs7641288', 0.21374321),\n",
       " ('rs1799659', 0.21364951),\n",
       " ('rs16960744', 0.21269256),\n",
       " ('rs7501812', 0.21245956),\n",
       " ('rs2972152', 0.21226692),\n",
       " ('rs899997', 0.21186876),\n",
       " ('rs12220743', 0.21171808),\n",
       " ('rs10811651', 0.2111587),\n",
       " ('rs9472777', 0.21045804),\n",
       " ('rs2351524', 0.2090433),\n",
       " ('rs181247', 0.20894027),\n",
       " ('rs944766', 0.20708418),\n",
       " ('rs504691', 0.2069099),\n",
       " ('rs12207548', 0.20604324),\n",
       " ('rs12411886', 0.20568395),\n",
       " ('rs1333037', 0.20565629),\n",
       " ('rs9307934', 0.20534721),\n",
       " ('rs6719001', 0.20431301),\n",
       " ('rs2890769', 0.20354795),\n",
       " ('rs967545', 0.20335072),\n",
       " ('rs4374101', 0.20309764),\n",
       " ('rs2184833', 0.20303643),\n",
       " ('rs16991453', 0.2027145),\n",
       " ('rs6589564', 0.20268404),\n",
       " ('rs12449964', 0.2025984),\n",
       " ('rs8066372', 0.2019968),\n",
       " ('rs7182567', 0.20169896),\n",
       " ('rs10744777', 0.20145094),\n",
       " ('rs7528419', 0.20141244),\n",
       " ('rs3752705', 0.20085168),\n",
       " ('rs1537373', 0.19989944),\n",
       " ('rs10965212', 0.19984257),\n",
       " ('rs7049105', 0.19928658),\n",
       " ('rs4790885', 0.199054),\n",
       " ('rs2404715', 0.19904864),\n",
       " ('rs8089632', 0.19895545),\n",
       " ('rs8109627', 0.19879127),\n",
       " ('rs394352', 0.19874072),\n",
       " ('rs9351817', 0.19853532),\n",
       " ('rs7278845', 0.19780147),\n",
       " ('rs4257260', 0.19760609),\n",
       " ('rs216183', 0.1975987),\n",
       " ('rs963052', 0.1975615),\n",
       " ('rs10793514', 0.19650882),\n",
       " ('rs2954029', 0.19636786),\n",
       " ('rs2053086', 0.19617128),\n",
       " ('rs12695685', 0.19560814),\n",
       " ('rs6723704', 0.19501114),\n",
       " ('rs11871738', 0.19375587),\n",
       " ('rs16911231', 0.19321),\n",
       " ('rs11650649', 0.19318783),\n",
       " ('rs7030641', 0.19318128),\n",
       " ('rs7350481', 0.1928603),\n",
       " ('rs2259820', 0.19279528),\n",
       " ('rs9319295', 0.19279096),\n",
       " ('rs11655813', 0.19241631),\n",
       " ('rs1199337', 0.19210005),\n",
       " ('rs7002680', 0.19182092),\n",
       " ('rs1405893', 0.19155478),\n",
       " ('rs11755825', 0.1915065),\n",
       " ('rs737280', 0.19130611),\n",
       " ('rs4256090', 0.1908989),\n",
       " ('rs12903203', 0.19000363),\n",
       " ('rs1632484', 0.18991649),\n",
       " ('rs13423088', 0.18943036),\n",
       " ('rs10883808', 0.18878841),\n",
       " ('rs7979473', 0.18813062),\n",
       " ('rs7225168', 0.18794182),\n",
       " ('rs3848460', 0.18757457),\n",
       " ('rs16893614', 0.1871593),\n",
       " ('rs1005902', 0.18704247),\n",
       " ('rs4921915', 0.18675208),\n",
       " ('rs9913504', 0.18649101),\n",
       " ('rs4509829', 0.1863904),\n",
       " ('rs6919980', 0.18638897),\n",
       " ('rs320', 0.18638623),\n",
       " ('rs1720819', 0.18563843),\n",
       " ('rs7245398', 0.18508887),\n",
       " ('rs2682539', 0.18504429),\n",
       " ('rs6805498', 0.18437004),\n",
       " ('rs620828', 0.18431127),\n",
       " ('rs169210', 0.18347394),\n",
       " ('rs8180558', 0.1832613),\n",
       " ('rs7319344', 0.18303287),\n",
       " ('rs2077750', 0.18247724),\n",
       " ('rs12190287', 0.18216991),\n",
       " ('rs1746050', 0.1820383),\n",
       " ('rs999474', 0.18171918),\n",
       " ('rs10116277', 0.1816349),\n",
       " ('rs4767293', 0.18135798),\n",
       " ('rs4634932', 0.18080282),\n",
       " ('rs1169300', 0.18058133),\n",
       " ('rs4714951', 0.18053293),\n",
       " ('rs9369640', 0.1799016),\n",
       " ('rs884957', 0.17743587),\n",
       " ('rs634963', 0.17736554),\n",
       " ('rs1567442', 0.17615855),\n",
       " ('rs1183910', 0.17395973),\n",
       " ('rs2105092', 0.17375588),\n",
       " ('rs143499', 0.17351007),\n",
       " ('rs10757278', 0.17325497),\n",
       " ('rs12327574', 0.17321885),\n",
       " ('rs1713795', 0.17293209),\n",
       " ('rs7314282', 0.17121252),\n",
       " ('rs1333040', 0.17078829),\n",
       " ('rs10883806', 0.17008781),\n",
       " ('rs4707922', 0.16958934),\n",
       " ('rs11914992', 0.16949153),\n",
       " ('rs2661834', 0.16872752),\n",
       " ('rs608848', 0.16872013),\n",
       " ('rs11191557', 0.16864562),\n",
       " ('rs2126202', 0.16842937),\n",
       " ('rs12526453', 0.16832352),\n",
       " ('rs8068175', 0.16762644),\n",
       " ('rs1333048', 0.16744554),\n",
       " ('rs2834430', 0.16675264),\n",
       " ('rs6586891', 0.16674262),\n",
       " ('rs17115100', 0.1660335),\n",
       " ('rs2760736', 0.16594219),\n",
       " ('rs10738607', 0.16558456),\n",
       " ('rs739468', 0.16555613),\n",
       " ('rs4767284', 0.16482809),\n",
       " ('rs7132172', 0.16478452),\n",
       " ('rs1004638', 0.16477168),\n",
       " ('rs6475606', 0.16448402),\n",
       " ('rs1931646', 0.16279343),\n",
       " ('rs7742443', 0.16256857),\n",
       " ('rs2639463', 0.16254771),\n",
       " ('rs4714990', 0.16147041),\n",
       " ('rs4673558', 0.16108364),\n",
       " ('rs2169357', 0.16082),\n",
       " ('rs11879562', 0.16045344),\n",
       " ('rs7278204', 0.15987664),\n",
       " ('rs7739181', 0.15797496),\n",
       " ('rs1333045', 0.15796924),\n",
       " ('rs17612742', 0.15548825),\n",
       " ('rs2889543', 0.15543103),\n",
       " ('rs7091108', 0.15469551),\n",
       " ('rs765467', 0.15421009),\n",
       " ('rs7651039', 0.15417194),\n",
       " ('rs7035484', 0.15396905),\n",
       " ('rs9349379', 0.15357348),\n",
       " ('rs3120137', 0.15351966),\n",
       " ('rs11072793', 0.15333891),\n",
       " ('rs7044859', 0.15164757),\n",
       " ('rs1333046', 0.15158504),\n",
       " ('rs10806731', 0.15140915),\n",
       " ('rs3786725', 0.15135813),\n",
       " ('rs1567471', 0.1512034),\n",
       " ('rs10120688', 0.15093493),\n",
       " ('rs1231206', 0.15050626),\n",
       " ('rs2244309', 0.15014505),\n",
       " ('rs11191543', 0.14949721),\n",
       " ('rs6807945', 0.14933974),\n",
       " ('rs11191558', 0.14917696),\n",
       " ('rs2377056', 0.14911795),\n",
       " ('rs643319', 0.14911473),\n",
       " ('rs13194360', 0.14911222),\n",
       " ('rs3847953', 0.1490959),\n",
       " ('rs1495743', 0.1490022),\n",
       " ('rs3918285', 0.14887595),\n",
       " ('rs3919447', 0.14816618),\n",
       " ('rs2954021', 0.14788455),\n",
       " ('rs12985339', 0.14775562),\n",
       " ('rs4937126', 0.14775443),\n",
       " ('rs2252383', 0.14761662),\n",
       " ('rs2096066', 0.14739132),\n",
       " ('rs1727895', 0.1472395),\n",
       " ('rs593226', 0.14723444),\n",
       " ('rs952227', 0.14707005),\n",
       " ('rs1671019', 0.14693308),\n",
       " ('rs2281727', 0.14672852),\n",
       " ('rs2227564', 0.14652199),\n",
       " ('rs12682131', 0.14626408),\n",
       " ('rs622472', 0.14616013),\n",
       " ('rs8181050', 0.1459468),\n",
       " ('rs1889018', 0.1455872),\n",
       " ('rs4778909', 0.1452992),\n",
       " ('rs599839', 0.1451695),\n",
       " ('rs7127880', 0.14498878),\n",
       " ('rs11634042', 0.14465135),\n",
       " ('rs6457792', 0.14459538),\n",
       " ('rs2943632', 0.14454651),\n",
       " ('rs11065385', 0.14389235),\n",
       " ('rs2160669', 0.14366663),\n",
       " ('rs10811647', 0.14361036),\n",
       " ('rs8131284', 0.14342237),\n",
       " ('rs11937849', 0.14322233),\n",
       " ('rs10509759', 0.14295566),\n",
       " ('rs10762786', 0.14240098),\n",
       " ('rs1360579', 0.14176309),\n",
       " ('rs1865051', 0.14171028),\n",
       " ('rs12945496', 0.14126348),\n",
       " ('rs7177699', 0.1411643),\n",
       " ('rs2315065', 0.14089143),\n",
       " ('rs11925809', 0.14087075),\n",
       " ('rs2895811', 0.14032793),\n",
       " ('rs3740390', 0.14014924),\n",
       " ('rs11066301', 0.13993752),\n",
       " ('rs4409766', 0.13986492),\n",
       " ('rs2026458', 0.13966787),\n",
       " ('rs12193973', 0.13958621),\n",
       " ('rs10850024', 0.13917542),\n",
       " ('rs2327429', 0.13914096),\n",
       " ('rs3125050', 0.13912898),\n",
       " ('rs1556515', 0.13912544),\n",
       " ('rs1713809', 0.1390323),\n",
       " ('rs1463217', 0.13896865),\n",
       " ('rs8105901', 0.13884318),\n",
       " ('rs2069418', 0.13845086),\n",
       " ('rs1527798', 0.13837385),\n",
       " ('rs2457574', 0.13834786),\n",
       " ('rs1056854', 0.13766217),\n",
       " ('rs1412751', 0.13761634),\n",
       " ('rs7857345', 0.13681889),\n",
       " ('rs10455872', 0.13645768),\n",
       " ('rs4766453', 0.13609684),\n",
       " ('rs2063346', 0.13603437),\n",
       " ('rs703999', 0.13503623),\n",
       " ('rs579459', 0.13494456),\n",
       " ('rs7630657', 0.13480127),\n",
       " ('rs216206', 0.13436937),\n",
       " ('rs10048206', 0.13393676),\n",
       " ('rs8132042', 0.13350558),\n",
       " ('rs7966915', 0.13319802),\n",
       " ('rs9402538', 0.13294733),\n",
       " ('rs4766566', 0.13276875),\n",
       " ('rs3971872', 0.1326853),\n",
       " ('rs11881940', 0.13241506),\n",
       " ('rs13197670', 0.1319347),\n",
       " ('rs1285991', 0.1312424),\n",
       " ('rs9375992', 0.13119122),\n",
       " ('rs2151280', 0.1311127),\n",
       " ('rs1169288', 0.13110185),\n",
       " ('rs12603592', 0.13087487),\n",
       " ('rs7173743', 0.13073349),\n",
       " ('rs4378452', 0.13057837),\n",
       " ('rs518594', 0.13009906),\n",
       " ('rs12544096', 0.12963927),\n",
       " ('rs11067009', 0.12928557),\n",
       " ('rs1881409', 0.12832165),\n",
       " ('rs199623', 0.12831318),\n",
       " ('rs1727889', 0.12780023),\n",
       " ('rs7649849', 0.12777114),\n",
       " ('rs9402543', 0.12762642),\n",
       " ('rs7098825', 0.12754059),\n",
       " ('rs3088442', 0.12740803),\n",
       " ('rs4925092', 0.12719595),\n",
       " ('rs501120', 0.12709984),\n",
       " ('rs1063192', 0.12683398),\n",
       " ('rs199629', 0.12602812),\n",
       " ('rs7933887', 0.12600413),\n",
       " ('rs12190423', 0.12595499),\n",
       " ('rs4624107', 0.1257236),\n",
       " ('rs6841581', 0.12562454),\n",
       " ('rs13208076', 0.12557387),\n",
       " ('rs9559759', 0.12537944),\n",
       " ('rs944801', 0.12508959),\n",
       " ('rs11556924', 0.12493718),\n",
       " ('rs4244602', 0.12463379),\n",
       " ('rs7181240', 0.124344885),\n",
       " ('rs7769085', 0.12418765),\n",
       " ('rs10738610', 0.1241622),\n",
       " ('rs3742375', 0.12341857),\n",
       " ('rs11191472', 0.12309384),\n",
       " ('rs2834431', 0.12307334),\n",
       " ('rs10774625', 0.122627735),\n",
       " ('rs17328201', 0.12249732),\n",
       " ('rs11721947', 0.12239337),\n",
       " ('rs4470201', 0.1219427),\n",
       " ('rs6589565', 0.12174606),\n",
       " ('rs4456618', 0.12150392),\n",
       " ('rs7922269', 0.12131679),\n",
       " ('rs12938295', 0.1205692),\n",
       " ('rs7028570', 0.12035227),\n",
       " ('rs9974878', 0.120093346),\n",
       " ('rs8065650', 0.12008119),\n",
       " ('rs3796592', 0.11994994),\n",
       " ('rs9351814', 0.11982626),\n",
       " ('rs8074850', 0.119725704),\n",
       " ('rs1626280', 0.11954808),\n",
       " ('rs7212538', 0.11948857),\n",
       " ('rs2649999', 0.119405925),\n",
       " ('rs684666', 0.118973434),\n",
       " ('rs10278591', 0.11890599),\n",
       " ('rs2410620', 0.11872053),\n",
       " ('rs4925114', 0.1183902),\n",
       " ('rs4766517', 0.11823702),\n",
       " ('rs9395190', 0.11763847),\n",
       " ('rs6490029', 0.11754608),\n",
       " ('rs12052201', 0.11733836),\n",
       " ('rs1383634', 0.116570115),\n",
       " ('rs4408839', 0.1165185),\n",
       " ('rs10788792', 0.11625886),\n",
       " ('rs1556516', 0.1157012),\n",
       " ('rs2048327', 0.11562371),\n",
       " ('rs7895842', 0.11532092),\n",
       " ('rs4306376', 0.115258306),\n",
       " ('rs559469', 0.11523008),\n",
       " ('rs9830762', 0.11517358),\n",
       " ('rs1332844', 0.11492205),\n",
       " ('rs12609863', 0.114899874),\n",
       " ('rs4766451', 0.11467329),\n",
       " ('rs2293251', 0.114209056),\n",
       " ('rs13202946', 0.11340237),\n",
       " ('rs199630', 0.11335164),\n",
       " ('rs9375990', 0.113140464),\n",
       " ('rs2106119', 0.11263883),\n",
       " ('rs12899940', 0.11208463),\n",
       " ('rs3918286', 0.112039804),\n",
       " ('rs12207772', 0.11196911),\n",
       " ('rs7248164', 0.11154616),\n",
       " ('rs264', 0.11147511),\n",
       " ('rs2818912', 0.11105293),\n",
       " ('rs16911261', 0.11081034),\n",
       " ('rs2266788', 0.11066127),\n",
       " ('rs4662414', 0.11023259),\n",
       " ('rs8032771', 0.10968372),\n",
       " ('rs1265142', 0.10935092),\n",
       " ('rs13027529', 0.10933316),\n",
       " ('rs10028123', 0.109297276),\n",
       " ('rs1182933', 0.10911012),\n",
       " ('rs15563', 0.10889852),\n",
       " ('rs6722332', 0.108756095),\n",
       " ('rs750987', 0.108486414),\n",
       " ('rs1008878', 0.108070076),\n",
       " ('rs4887078', 0.10796261),\n",
       " ('rs825030', 0.107494),\n",
       " ('rs1004467', 0.1072793),\n",
       " ('rs2393791', 0.106488764),\n",
       " ('rs1322764', 0.1059463),\n",
       " ('rs11191416', 0.10517043),\n",
       " ('rs1380283', 0.10477078),\n",
       " ('rs4252120', 0.10465789),\n",
       " ('rs12740374', 0.10455668),\n",
       " ('rs10774613', 0.10406947),\n",
       " ('rs2641431', 0.103847384),\n",
       " ('rs17066175', 0.103780985),\n",
       " ('rs10168404', 0.10343897),\n",
       " ('rs1727885', 0.10339975),\n",
       " ('rs7769943', 0.103334665),\n",
       " ('rs2576355', 0.10323784),\n",
       " ('rs3217992', 0.10312444),\n",
       " ('rs2457179', 0.10255498),\n",
       " ('rs2306556', 0.10237551),\n",
       " ('rs1534650', 0.10234702),\n",
       " ('rs11921193', 0.10210633),\n",
       " ('rs17562391', 0.10200167),\n",
       " ('rs10933436', 0.101881266),\n",
       " ('rs9326246', 0.10161239),\n",
       " ('rs6590202', 0.101524115),\n",
       " ('rs12413409', 0.10121429),\n",
       " ('rs2075290', 0.1011219),\n",
       " ('rs2083636', 0.10028219),\n",
       " ('rs7196903', 0.10008228),\n",
       " ('rs1387774', 0.09976506),\n",
       " ('rs17091871', 0.09902191),\n",
       " ('rs12593043', 0.098947525),\n",
       " ('rs11191454', 0.098908186),\n",
       " ('rs17609940', 0.098890424),\n",
       " ('rs11654081', 0.09875703),\n",
       " ('rs11667905', 0.09850073),\n",
       " ('rs573141', 0.09823376),\n",
       " ('rs2504927', 0.09804469),\n",
       " ('rs886126', 0.097794056),\n",
       " ('rs2875497', 0.09776282),\n",
       " ('rs11066320', 0.09774572),\n",
       " ('rs11072811', 0.09760782),\n",
       " ('rs511586', 0.09742159),\n",
       " ('rs2103325', 0.09734875),\n",
       " ('rs10883815', 0.097170115),\n",
       " ('rs7968960', 0.09698701),\n",
       " ('rs10758574', 0.0966804),\n",
       " ('rs2106120', 0.09663367),\n",
       " ('rs671765', 0.09652305),\n",
       " ('rs7258749', 0.096108556),\n",
       " ('rs8032552', 0.09548533),\n",
       " ('rs7454157', 0.095216274),\n",
       " ('rs2166529', 0.095210195),\n",
       " ('rs1727904', 0.094947815),\n",
       " ('rs1967917', 0.094646156),\n",
       " ('rs2823152', 0.09456134),\n",
       " ('rs13196617', 0.09338951),\n",
       " ('rs7751826', 0.09295213),\n",
       " ('rs1052299', 0.092806816),\n",
       " ('rs969282', 0.09194851),\n",
       " ('rs1535616', 0.09170234),\n",
       " ('rs9402540', 0.09157717),\n",
       " ('rs2063347', 0.09124899),\n",
       " ('rs7181432', 0.090595245),\n",
       " ('rs9296495', 0.090563536),\n",
       " ('rs1974369', 0.090138555),\n",
       " ('rs10757269', 0.09007716),\n",
       " ('rs2297472', 0.08993924),\n",
       " ('rs12356322', 0.08988273),\n",
       " ('rs195203', 0.08986205),\n",
       " ('rs2184717', 0.08968395),\n",
       " ('rs182106', 0.089657545),\n",
       " ('rs1294449', 0.08934045),\n",
       " ('rs12524865', 0.08826095),\n",
       " ('rs4925123', 0.088249624),\n",
       " ('rs9473086', 0.08798504),\n",
       " ('rs9880964', 0.08794919),\n",
       " ('rs10807323', 0.08776587),\n",
       " ('rs2244608', 0.08765745),\n",
       " ('rs11633170', 0.08714199),\n",
       " ('rs1881410', 0.08647144),\n",
       " ('rs12980480', 0.086410046),\n",
       " ('rs3213545', 0.08588815),\n",
       " ('rs1360590', 0.08573961),\n",
       " ('rs7140084', 0.08564365),\n",
       " ('rs4762452', 0.08552313),\n",
       " ('rs679038', 0.085392),\n",
       " ('rs1830319', 0.08504844),\n",
       " ('rs2184061', 0.08496553),\n",
       " ('rs602633', 0.084879875),\n",
       " ('rs6903612', 0.084266424),\n",
       " ('rs2395656', 0.08421928),\n",
       " ('rs1567438', 0.0840286),\n",
       " ('rs2339973', 0.08396649),\n",
       " ('rs534079', 0.0836547),\n",
       " ('rs2393775', 0.083496094),\n",
       " ('rs216191', 0.08344078),\n",
       " ('rs9911672', 0.083369136),\n",
       " ('rs8043123', 0.083242655),\n",
       " ('rs7280612', 0.08274078),\n",
       " ('rs11634628', 0.08273101),\n",
       " ('rs2624695', 0.08255032),\n",
       " ('rs6490025', 0.08249533),\n",
       " ('rs10490320', 0.08227563),\n",
       " ('rs2972153', 0.0822072),\n",
       " ('rs950966', 0.08217394),\n",
       " ('rs1537370', 0.08216596),\n",
       " ('rs11670205', 0.08209205),\n",
       " ('rs4766578', 0.08178973),\n",
       " ('rs11078883', 0.080964684),\n",
       " ('rs6747275', 0.080586016),\n",
       " ('rs7752775', 0.08056635),\n",
       " ('rs2113607', 0.08022207),\n",
       " ('rs9900673', 0.080117226),\n",
       " ('rs2327620', 0.08009392),\n",
       " ('rs11238818', 0.079930305),\n",
       " ('rs7977534', 0.07941276),\n",
       " ('rs9876505', 0.07928157),\n",
       " ('rs642898', 0.0791772),\n",
       " ('rs10811641', 0.07798433),\n",
       " ('rs8051680', 0.07785392),\n",
       " ('rs7616575', 0.07745391),\n",
       " ('rs890267', 0.07717383),\n",
       " ('rs2076828', 0.0770998),\n",
       " ('rs10793513', 0.076630354),\n",
       " ('rs7139492', 0.0764544),\n",
       " ('rs3744115', 0.075972795),\n",
       " ('rs7972222', 0.0758872),\n",
       " ('rs12602348', 0.07511228),\n",
       " ('rs12220375', 0.07507908),\n",
       " ('rs8103839', 0.07505),\n",
       " ('rs6842241', 0.07472235),\n",
       " ('rs11078865', 0.07471746),\n",
       " ('rs11072810', 0.07465923),\n",
       " ('rs1231209', 0.074644566),\n",
       " ('rs17114046', 0.07440323),\n",
       " ('rs9296487', 0.07384503),\n",
       " ('rs7605484', 0.073578656),\n",
       " ('rs535949', 0.07345247),\n",
       " ('rs9319286', 0.073376656),\n",
       " ('rs2984943', 0.072794735),\n",
       " ('rs13186205', 0.072588444),\n",
       " ('rs12197124', 0.07167745),\n",
       " ('rs376563', 0.071269155),\n",
       " ('rs11066188', 0.0709908),\n",
       " ('rs2176042', 0.07094216),\n",
       " ('rs9375987', 0.07079297),\n",
       " ('rs1608152', 0.06954372),\n",
       " ('rs10511701', 0.06844008),\n",
       " ('rs9830630', 0.0684194),\n",
       " ('rs12486823', 0.06839776),\n",
       " ('rs2489957', 0.06831038),\n",
       " ('rs12943500', 0.0682956),\n",
       " ('rs17611937', 0.06817481),\n",
       " ('rs660240', 0.068142414),\n",
       " ('rs7930786', 0.06813872),\n",
       " ('rs12413046', 0.06773591),\n",
       " ('rs651821', 0.06768501),\n",
       " ('rs7769954', 0.06765485),\n",
       " ('rs9463106', 0.06758684),\n",
       " ('rs46522', 0.067457855),\n",
       " ('rs7692387', 0.06733465),\n",
       " ('rs3781285', 0.067254305),\n",
       " ('rs8034028', 0.06719285),\n",
       " ('rs9970807', 0.066899896),\n",
       " ('rs11191593', 0.06594032),\n",
       " ('rs9905529', 0.06577909),\n",
       " ('rs7560547', 0.06521106),\n",
       " ('rs4510208', 0.0651381),\n",
       " ('rs11643561', 0.06509745),\n",
       " ('rs10509079', 0.06489062),\n",
       " ('rs2277545', 0.06421238),\n",
       " ('rs510785', 0.064171135),\n",
       " ('rs10847503', 0.063866735),\n",
       " ('rs11680332', 0.06384325),\n",
       " ('rs7762949', 0.063519),\n",
       " ('rs4977756', 0.06266892),\n",
       " ('rs6503324', 0.06195879),\n",
       " ('rs4741921', 0.060936242),\n",
       " ('rs1360589', 0.060620666),\n",
       " ('rs11807450', 0.060475588),\n",
       " ('rs10899973', 0.06028378),\n",
       " ('rs735396', 0.06025207),\n",
       " ('rs6453543', 0.06005168),\n",
       " ('rs6502622', 0.059504986),\n",
       " ('rs541483', 0.059461594),\n",
       " ('rs9351813', 0.059300542),\n",
       " ('rs10814871', 0.058951765),\n",
       " ('rs11993784', 0.05877033),\n",
       " ('rs4415546', 0.05863452),\n",
       " ('rs9982601', 0.057847023),\n",
       " ('rs7634965', 0.057336807),\n",
       " ('rs6589566', 0.0568462),\n",
       " ('rs10790162', 0.055743396),\n",
       " ('rs7816032', 0.055700064),\n",
       " ('rs1878406', 0.055464983),\n",
       " ('rs12602764', 0.054650486),\n",
       " ('rs9632884', 0.05380535),\n",
       " ('rs6801296', 0.053655624),\n",
       " ('rs199622', 0.053561628),\n",
       " ('rs12491701', 0.053364515),\n",
       " ('rs684521', 0.050460935),\n",
       " ('rs8065563', 0.04980737),\n",
       " ('rs11191447', 0.04916525),\n",
       " ('rs263', 0.04887864),\n",
       " ('rs11191479', 0.048045874),\n",
       " ('rs684196', 0.047831357),\n",
       " ('rs573687', 0.04731244),\n",
       " ('rs343', 0.047307253),\n",
       " ('rs10457618', 0.047043562),\n",
       " ('rs9521678', 0.046875),\n",
       " ('rs3786722', 0.046649575),\n",
       " ('rs9838412', 0.04442203),\n",
       " ('rs9641666', 0.044298887),\n",
       " ('rs2381686', 0.044086456),\n",
       " ('rs9321421', 0.04373145),\n",
       " ('rs7865618', 0.0420202),\n",
       " ('rs1537378', 0.041097373),\n",
       " ('rs11107829', 0.039687157),\n",
       " ('rs615552', 0.039513826),\n",
       " ('rs2410621', 0.03928554),\n",
       " ('rs478392', 0.038311243),\n",
       " ('rs4252109', 0.038193703),\n",
       " ('rs1830321', 0.037250817),\n",
       " ('rs7750679', 0.037008703),\n",
       " ('rs1537374', 0.03670156),\n",
       " ('rs6924150', 0.036239028),\n",
       " ('rs3845800', 0.035285473),\n",
       " ('rs9824909', 0.034018695),\n",
       " ('rs2457564', 0.033850968),\n",
       " ('rs1199338', 0.03323695),\n",
       " ('rs9906500', 0.033176005),\n",
       " ('rs1534385', 0.033162355),\n",
       " ('rs4515014', 0.032981217),\n",
       " ('rs8181047', 0.032056034),\n",
       " ('rs9824878', 0.031782627),\n",
       " ('rs1011970', 0.030145764),\n",
       " ('rs2221181', 0.029934764),\n",
       " ('rs11191511', 0.029394448),\n",
       " ('rs10811640', 0.02914548),\n",
       " ('rs674302', 0.028367937),\n",
       " ('rs2244273', 0.027947932),\n",
       " ('rs479596', 0.02658847),\n",
       " ('rs2069416', 0.025765866),\n",
       " ('rs1727894', 0.025466442),\n",
       " ('rs1727881', 0.025318593),\n",
       " ('rs1537375', 0.023893356),\n",
       " ('rs7139880', 0.023126721),\n",
       " ('rs4675310', 0.019256115),\n",
       " ('rs2036927', 0.01845932)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(snp_to_max_val.items(), key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot distribution\n",
    "plt.hist(snp_to_max_val.values())\n",
    "plt.title(\"Distribution of maximum predicted change due to SNP\")\n",
    "plt.xlabel(\"maximum absolute change\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(snp_to_max_val.items(), key=lambda tup: tup[1], reverse=True)[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
